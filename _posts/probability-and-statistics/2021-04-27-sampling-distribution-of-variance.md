---
title: "Sampling Distribution of Variance"
layout: post
use_math: true
tags: ["Probability"]
---

### ì„œë¡ 
2021-1í•™ê¸°, ëŒ€í•™ì—ì„œ 'í™•ë¥ ê³¼ í†µê³„' ìˆ˜ì—…ì„ ë“£ê³  ê³µë¶€í•œ ë°”ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

<br><span class="statement-title">TOC.</span><br>

- Sampling Distribution of the difference btw two mean
- Sampling Distribution of $S^2$

<hr/>

### Sampling Distribution of the difference btw two mean

ì´ë²ˆì—ëŠ” ë‘ ê°œì˜ ì„œë¡œ populationì—ì„œ ë½‘ì€ ë‘ independent sampleì„ ìƒê°í•´ë³´ì!

Let $X_1, \dots, X_{n_1}$, and $Y_1, \dots, Y_{n_2}$ be two independent random samples with $E[X_1] = \mu_1$, $\text{Var}(X_1) = \sigma_1^2$, and  $E[X_2] = \mu_2$, $\text{Var}(Y_2) = \sigma_2^2$.

ìš°ë¦¬ëŠ” "ë‘ ìƒ˜í”Œ í‰ê· ì˜ ì°¨" $\mu_1 - \mu_2$ì— ëŒ€í•œ inferenceë¥¼ ìˆ˜í–‰í•˜ê³ ì í•œë‹¤. ì´ë•Œ, $\overline{X} - \overline{Y}$ë¥¼ ì‚¬ìš©í•˜ë©´ "ë‘ ìƒ˜í”Œ í‰ê· ì˜ ì°¨"ì— ëŒ€í•´ ì¶”ë¡ í•  ìˆ˜ ìˆë‹¤!!

By CLT,

$$
\begin{aligned}
    \frac{\overline{X} - \mu_1}{\frac{\sigma_1}{\sqrt{n_1}}} \sim N(0, 1) \quad & \iff \quad \overline{X} \sim N\left(\mu_1, \frac{\sigma_1^2}{n_1}\right) \\
    \frac{\overline{Y} - \mu_2}{\frac{\sigma_2}{\sqrt{n_2}}} \sim N(0, 2) \quad & \iff \quad \overline{Y} \sim N\left(\mu_2, \frac{\sigma_2^2}{n_2}\right)
\end{aligned}
$$

ë”°ë¼ì„œ, $\overline{X} - \overline{Y}$ì— ëŒ€í•œ ë¶„í¬ëŠ” independentí•œ ë‘ normal distributionì— ëŒ€í•œ ë§ì…ˆìœ¼ë¡œ ì‰½ê²Œ ìœ ë„í•  ìˆ˜ ìˆë‹¤!

$$
\overline{X} - \overline{Y} \sim N\left( \mu_1 - \mu_2, \; \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2} \right)
$$

ìœ„ì˜ ì‚¬ì‹¤ì„ ì´ìš©í•´ì„œ, "ë‘ ìƒ˜í”Œ í‰ê· ì˜ ì°¨"ì— ëŒ€í•œ ì¶”ë¡ ë„ ì‰½ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤ ğŸ˜‰

<hr/>

### Sampling Distribution of $S^2$

Let $X_1, \dots, X_n$ be a random sample with $\text{Var}(X_i) = \sigma^2$. We already know that $E[S^2] = \sigma^2$. How about the distribution of $\displaystyle S^2 = \dfrac{1}{n-1} \sum^n_{i=1} (X_i - \overline{X})^2$?

ê°€ì¥ ê°„ë‹¨í•œ ê²½ìš°ì¸, $n=2$ì¸ ê²½ìš°ë¥¼ ì‚´í´ë³´ì. ì´ë•Œ, $\overline{X} = \dfrac{X_1 + X_2}{2}$ì´ë‹¤. ì´ë•Œ, $Y_i := X_i - \overline{X}$ë¼ê³  ë‘”ë‹¤ë©´,

$$
\begin{aligned}
    Y_1 = X_1 - \overline{X} = \frac{X_1 - X_2}{2} \\
    Y_2 = X_2 - \overline{X} = \frac{X_2 - X_1}{2} \\
\end{aligned}
$$

ì¦‰, $Y_1 = - Y_2$ë¡œ ì„œë¡œ dependentë‹¤! ê·¸ë˜ì„œ $S^2$ì— ëŒ€í•´ì„œëŠ” CLTë¥¼ ì ìš©í•  ìˆ˜ê°€ ì—†ë‹¤ ğŸ˜¥ ê·¸ëŸ¬ë‚˜ ì•„ë˜ì˜ ì •ë¦¬ë¥¼ ì˜ í™œìš©í•˜ë©´, $S^2$ì— ëŒ€í•œ Distributionì„ ìœ ë„í•  ìˆ˜ ìˆë‹¤!!

<br/>

<span class="statement-title">Note.</span><br>

Let $X_1, \dots, X_{n_1}$ be randome sample from $N(\mu, \sigma^2)$.

1\. $\overline{X} \sim N\left( \mu, \frac{\sigma^2}{n}\right)$

2\. $\displaystyle\sum^n_i \left( \frac{X_i - \mu}{\sigma} \right)^2 \sim \chi^2(n)$

<span class="statement-title">Theorem.</span> Sampling Distribution of $S^2$<br>

Let $X_1, \dots, X_{n_1}$ be randome sample from $N(\mu, \sigma^2)$, then

$$
\frac{(n-1)S^2}{\sigma^2} = \sum^n_{i=1} \left( \frac{X_i - \overline{X}}{\sigma}\right)^2 \sim \chi^2 (n-1)
$$

"We lose one degree of freedom, because we estimate a parameter $\mu$ by $\overline{X}$."

<span class="statement-title">*Proof*.</span><br>

<div class="math-statement" markdown="1">

$$
\begin{aligned}
\frac{1}{\sigma^2} \sum^n_i \left( X_i - \mu \right)^2 &= \frac{1}{\sigma^2} \sum^n_{i=1} (X_i - \overline{X} + \overline{X} - \mu)^2 \\
&= \frac{1}{\sigma^2} \sum^n_i (X_i - \overline{X})^2 + \frac{1}{\sigma^2} \sum^n_i (\overline{X} - \mu)^2 + \frac{1}{\sigma^2} \sum^n_i (X_i - \overline{X})(\overline{X} - \mu)
\end{aligned}
$$

ì´ë•Œ, ë§ˆì§€ë§‰ í…€ì¸ $\displaystyle \frac{1}{\sigma^2} \sum^n_i (X_i - \overline{X})(\overline{X} - \mu)$ë¥¼ ì‚´í´ë³´ì.

$$
\begin{aligned}
\frac{1}{\sigma^2} \sum^n_i (X_i - \overline{X})(\overline{X} - \mu) &= \frac{1}{\sigma^2} \cdot (\overline{X} - \mu) \cdot \sum^n_i (X_i - \overline{X}) \\
&= \frac{1}{\sigma^2} \cdot (\overline{X} - \mu) \cdot \cancelto{0}{(X_1 + \cdots + X_n - n\overline{X})} \\
&= 0
\end{aligned}
$$

ë‹¤ì‹œ ì›ë˜ì˜ ì‹ìœ¼ë¡œ ëŒì•„ê°€ì„œ

$$
\begin{aligned}
\frac{1}{\sigma^2} \sum^n_i \left( X_i - \mu \right)^2 
&= \frac{1}{\sigma^2} \sum^n_i (X_i - \overline{X})^2 + \frac{1}{\sigma^2} \sum^n_i (\overline{X} - \mu)^2 + \cancelto{0}{\frac{1}{\sigma^2} \sum^n_i (X_i - \overline{X})(\overline{X} - \mu)} \\
&= \frac{1}{\sigma^2} \sum^n_i (X_i - \overline{X})^2 + \frac{1}{\sigma^2} \sum^n_i (\overline{X} - \mu)^2 \\
&= \frac{1}{\sigma^2} \sum^n_i (X_i - \overline{X})^2 + \frac{n(\overline{X} - \mu)^2}{\sigma^2} \\
&= \frac{1}{\sigma^2} \sum^n_i (X_i - \overline{X})^2 + \left( \frac{\overline{X} - \mu}{\frac{\sigma}{\sqrt{n}}}\right)^2 \\
\end{aligned}
$$

ì´ë•Œ, ì¢Œë³€ì˜ $\displaystyle \frac{1}{\sigma^2} \sum^n_i \left( X_i - \mu \right)^2$ëŠ” $\chi^2(n)$ì˜ ë¶„í¬ë¥¼ ë”°ë¥´ê³ , ìš°ë³€ì˜ $\displaystyle \left( \frac{\overline{X} - \mu}{\frac{\sigma}{\sqrt{n}}}\right)^2$ëŠ” $\chi^2(1)$ì˜ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤.

ë§Œì•½ $Z = X + Y$ì—ì„œ $Z \sim \chi^2(n)$ì´ê³ , $Y \sim \chi^2(1)$ì´ê³ , ê²Œë‹¤ê°€ $X \perp Y$ë¼ë©´, $X \sim \chi^2(n-1)$ê°€ ëœë‹¤. ê·¸ëŸ¬ë‚˜ ìš°ë¦¬ëŠ” ì•„ì§ $X \perp Y$ì— ëŒ€í•´ ì–¸ê¸‰í•˜ì§€ ì•Šì•˜ë‹¤. ê·¸ë˜ì„œ ì•„ë˜ì˜ Lemmaë¥¼ í†µí•´ $X \perp Y$ë¥¼ í™•ì¸í•´ë³´ì.

<div class="statement" markdown="1">

<span class="statement-title">Lemma.</span><br>

Let $X_1, \dots, X_n$ be a random sample from $N(\mu, \sigma^2)$, then $S^2$ and $\overline{X}$ are independent.

In fact, $\overline{X}$ and $(X_1 - \overline{X}, \dots, X_n - \overline{X})$ are independent.

</div>

ë”°ë¼ì„œ, ìœ„ì˜ Lemmaì— ì˜í•´ 

$$
\frac{1}{\sigma^2} \sum^n_i (X_i - \overline{X})^2  = \frac{(n-1) S^2}{\sigma^2} \sim \chi^2(n-1)
$$

$\blacksquare$

</div>




