---
title: "Sampling Distribution"
layout: post
use_math: true
tags: ["Statistics"]
---

### ì„œë¡ 
2021-1í•™ê¸°, ëŒ€í•™ì—ì„œ 'í™•ë¥ ê³¼ í†µê³„' ìˆ˜ì—…ì„ ë“£ê³  ê³µë¶€í•œ ë°”ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

<br><span class="statement-title">TOC.</span><br>

- sample mean & sampling distribution
- random sample
- location measures of a sample
  - sample mean
  - sample median
  - sample mode
- variability measures of a sample
  - sample variance ğŸ”¥
  - sample standard deviation
  - range
- Distribution of Mean & CLT
- Weak Law of Large Numbers
- CLT; Central Limit Theorem


<hr/>

í™•í†µ ìˆ˜ì—…ì„ ë“£ëŠ” ì „ì²´ í•™ìƒì„ ëŒ€ìƒìœ¼ë¡œ, í™•í†µ ìˆ˜ì—…ì„ ì„ í˜¸í•˜ëŠ” í•™ìƒì˜ ë¹„ìœ¨ì„ êµ¬í•˜ê³ ì í•œë‹¤. ê·¸ëŸ°ë°, í™•í†µ ìˆ˜ì—…ì„ ë“£ëŠ” í•™ìƒ ìˆ˜ê°€ ë„ˆë¬´ ë§ì•„ì„œ ì „ì²´ë¥¼ ì¡°ì‚¬í•  ìˆœ ì—†ê³ , ì „ì²´ ì¤‘ $n$ëª… í•™ìƒì„ ëŒ€ìƒìœ¼ë¡œ ì„¤ë¬¸ì¡°ì‚¬ë¥¼ ì‹œí–‰í•œë‹¤ê³  í•˜ì.

$X$ê°€ $n$ëª…ì˜ í•™ìƒ ì¤‘ì— í™•í†µ ìˆ˜ì—…ì„ ì„ í˜¸í•œë‹¤ê³ , ì‘ë‹µí•œ í•™ìƒì— ëŒ€í•œ RVë¼ë©´, $X$ëŠ” HyperGeoì˜ ë¶„í¬ë¥¼ ë”°ë¥¼ ê²ƒì´ë‹¤.

ë˜, ë§Œì•½ ì „ì²´ í•™ìƒ ìˆ˜ê°€ ì¶©ë¶„íˆ í¬ë‹¤ë©´, HyperGeoë¥¼ BINìœ¼ë¡œ ê·¼ì‚¬í•  ìˆ˜ë„ ìˆì„ ê²ƒì´ë‹¤.

ì´ë•Œ, ê° í•™ìƒ $i$ì˜ ì„ í˜¸ë¥¼ RV $X_i$ë¡œ í‘œí˜„í•´ë³´ì. ê·¸ëŸ¬ë©´,

$$
X_i = \begin{cases}
    1 & i\text{-th student likes it!} \\
    0 & \text{else}
\end{cases}
$$

ê·¸ëŸ¬ë©´, ì „ì²´ RV $X_1, \dots, X_n$ë¥¼ ì¢…í•©í•˜ë©´, ìƒˆë¡œìš´ RV $\overline{X}$ë¥¼ ìœ ë„í•  ìˆ˜ ìˆë‹¤.

$$
\overline{X} := \frac{X_1 + \cdots X_n}{n}
$$

ìš°ë¦¬ëŠ” ì´ $\overline{X}$ë¥¼ \<**sample mean**\>ì´ë¼ê³  í•œë‹¤!

ìœ„ì˜ ì˜ˆì‹œë¥¼ ì¢€ë” êµ¬ì²´í™” í•´ì„œ ìƒê°í•´ë³´ì.

$n=100$, and 60 students said they like lecture. Then, $\overline{x} = \frac{60}{100} = 0.6$

ì´ë•Œ, ìš°ë¦¬ê°€ \<sample mean\> $\overline{x}$ì— ëŒ€í•´ ë…¼í•˜ê³ ì í•˜ëŠ” ì£¼ì œëŠ” ë°”ë¡œ

$$
P(\left| \overline{x} - 0.6 \right| < \epsilon)
$$

ê³¼ ê°™ì€ í™•ë¥ ì„ ì–´ë–»ê²Œ êµ¬í•˜ëŠ”ì§€ì— ëŒ€í•œ ê²ƒì´ë‹¤. ì´ê²ƒì„ êµ¬í•˜ê¸° ìœ„í•´ì„œëŠ” $\overline{x}$ì— ëŒ€í•œ ë¶„í¬ë¥¼ ì•Œì•„ì•¼ í•˜ë©°, ìš°ë¦¬ëŠ” ì´ê²ƒì„ \<**sampling distribution**\>ì´ë¼ê³  í•œë‹¤!

<hr/>

<span class="statement-title">Definition.</span> population<br>

A \<population\> is the totality of observations.

<span class="statement-title">Definition.</span> sample<br>

A \<sample\> is a subset of population.

<span class="statement-title">Definition.</span> random sample<br>

RVs $X_1, \dots, X_n$ are said to be a \<random sample\> of size $n$, <span class="half_HL">if they are independent and identically distributed as pmf or pdf $f(x)$</span>.


That is, 

$$
f_{(X_1, \dots, X_n)} (x_1, \dots, x_n) = f_{X_1} (x_1) \cdots f_{X_n} (x_n)
$$

The observed values $x_1, \dots, x_n$ of $X_1, \dots, X_n$ are called \<**sample points**\> or \<**observations**\>.

<hr/>

<span class="statement-title">Definition.</span> statistic<br>

A \<statistic\> is a function of a random sample $X_1, \dots, X_n$, <span class="half_HL">not depending on unknown parameters</span>.

<span class="statement-title">Example.</span><br>

Supp. $X_1, \dots, X_n$ is a random sample from $N(\mu, 1)$.

Then, 

1\. $\dfrac{X_1 + \cdots + X_n}{n}$ is a <u>statistic</u>!

2\. $\max \\{ X_1, \dots, X_n \\}$ is a <u>statistic</u>!

3\. $\dfrac{X_1 + \cdots + X_n + \mu}{n}$ is <u>not a statistic</u>!

ìš°ë¦¬ëŠ” \<statistic\>ë¼ëŠ” íŠ¹ë³„í•œ ì¡°ê±´ ì•„ë˜ì— ìˆì„ ë•Œ, random sampleì„ í†µí•´ populationì— ëŒ€í•œ inferenceë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.

<hr/>

#### Location Measures of a Sample

Let $X_1, \dots, X_n$ be a random sample.

<span class="statement-title">Definition.</span> sample mean<br>


$\overline{X} = \dfrac{X_1 + \cdots + X_n}{n}$ is called a \<sample mean\>.

(1) $\overline{X}$ is also a random varaible!

(2) If $E(X_1) = \mu$ and $\text{Var}(X_1) = \sigma^2$, then $E(\overline{X}) = \dfrac{n\mu}{n} = \mu$ and $\text{Var}(\overline{X}) = \dfrac{\sigma^2}{n}$

(3) $\overline{X}$ cab be sensitive to outliers.

<br/>

<span class="statement-title">Definition.</span> sample median<br>

ê·¸ëƒ¥ Sampleì—ì„œì˜ ì¤‘ê°„ê°’ì„ ë§í•¨.

<br/>

<span class="statement-title">Definition.</span> sample mode<br>

Sampleì—ì„œì˜ ìµœë¹ˆê°’.


<hr/>

#### Variability Measures of a Sample

Let $X_1, \dots, X_n$ be a random sample with $E[X_i] = \mu$ and $\text{Var}(X_i) = \sigma^2$.


<span class="statement-title">Definition.</span> sample variance<br>


$$
S^2 := \frac{1}{n-1} \sum^n_{i=1} \left( X_i - \overline{X}\right)^2
$$

Q. Why $(n-1)$ in the bottom??

A. ì™œëƒí•˜ë©´, $(n-1)$ë¡œ ë‚˜ëˆ ì¤˜ì•¼ $E[S^2]$ì´ $\sigma^2$ì´ ë˜ê¸° ë•Œë¬¸!!!

<span class="statement-title">*Proof*.</span><br>

<div class="math-statement" markdown="1">

w.l.o.g. we can assume that $E[X_i] = 0$. (ê·¸ëƒ¥ í¸ì˜ë¥¼ ìœ„í•´ $X_i$ë¥¼ ì ë‹¹íˆ í‘œì¤€í™” í–ˆë‹¤ê³  ë³´ë©´ ëœë‹¤.)

$$
\begin{aligned}
S^2 &= \frac{1}{n-1} \sum^n_{i=1} \left( X_i^2 - 2 X_i \overline{X} + (\overline{X})^2 \right) \\
    &= \frac{1}{n-1} \left\{ \sum^n_{i=1} X_i^2 - 2 \overline{X} \sum^n_{i=1} X_i + n (\overline{X})^2 \right\} \\
\end{aligned}
$$

ì´ë•Œ, $\displaystyle\sum^n_{i=1} X_i$ëŠ” ê·¸ ì •ì˜ì— ì˜í•´ $n\overline{X}$ê°€ ëœë‹¤.

$$
\begin{aligned}
S^2 &= \frac{1}{n-1} \left\{ \sum^n_{i=1} X_i^2 - 2 \overline{X} \cdot n\overline{X} + n (\overline{X})^2 \right\} \\
    &= \frac{1}{n-1} \left\{ \sum^n_{i=1} X_i^2 - n (\overline{X})^2 \right\} \\
\end{aligned}
$$

ì´ì œ ìœ„ì˜ ì‹ì˜ ì–‘ë³€ì— í‰ê· ì„ ì·¨í•´ë³´ì.

$$
\begin{aligned}
E[S^2] &= \frac{1}{n-1} \left\{ \sum^n_{i=1} E(X_i)^2 - n E\left[(\overline{X})^2\right] \right\} \\
       &= \frac{1}{n-1} \left\{ n \cdot \sigma^2 - n \cdot \frac{1}{n^2} \cdot E \left[(X_1 + \cdots + X_n)^2 \right] \right\} \\
       &= \frac{1}{n-1} \left\{ n \cdot \sigma^2 - \frac{1}{n} \cdot \left( n E[X_1^2] + \cancelto{0}{E[X_i X_j]} + \cdots \right) \right\} \\
       &= \frac{1}{n-1} \left\{ n \cdot \sigma^2 - \frac{1}{\cancel{n}} \cdot \left( \cancel{n} \cancelto{\sigma^2}{E[X_1^2]} \right) \right\} \quad (\text{independence}) \\
       &= \frac{1}{n-1} \left\{ n \cdot \sigma^2 - \sigma^2 \right\} \\
       &= \sigma^2
\end{aligned}
$$

$\blacksquare$

</div>

<br/>

<span class="statement-title">Definition.</span> sample standard deviation<br>

$$
S := \sqrt{S^2} = \sqrt{\frac{1}{n-1} \sum^n_{i=1}\left( X_i - \overline{X} \right)^2}
$$

<br/>

<span class="statement-title">Definition.</span> range<br>

$$
R := \max_{1 \le i \le n} X_i - \min_{1 \le i \le n} X_i
$$

<hr/>

<span class="statement-title">Definition.</span> sample distribution<br>

The <span class="half_HL">probability distribution of a statistic</span> is called a \<sample distribution\>.

ex) distribution of sample mean, distribution of sample variance, ...

#### Sampling Distribution of Means and CLT

Let $X_1, \dots, X_n$ be a random sample with $E[X_i] = \mu$ and $\text{Var}(X_i) = \sigma^2$.

Then,

- $E[\overline{X}] = \mu$
- $\text{Var}(\overline{X}) = E\left[\left(\overline{X} - E[\overline{X}]\right)^2 \right] = \dfrac{\sigma^2}{n}$

ì´ë•Œ, \<CLT; Central Limit Theorem\>ì€ $n$ì´ ë¬´í•œìœ¼ë¡œ ê°ˆë•Œ, $\text{Var}(\overline{X}) = \dfrac{\sigma^2}{n}$ì´ 0ì´ ìˆ˜ë ´í•¨ì„ ê¸°ìˆ í•œë‹¤. ê·¸ë¦¬ê³  ì´ì— ë”°ë¼, $\overline{X} \rightarrow \mu$ê°€ ëœë‹¤.

<hr/>

#### Weak Law of Large Numbers

<span class="statement-title">Theorem.</span> WLLN<br>

Let $X_1, \dots, X_n$ be a random sample with $E[X_i] = \mu$ and $\text{Var}(X_i) = \sigma^2$.

Let $\overline{X} = \dfrac{X_1 + \cdots + X_n}{n}$.

For any $\epsilon > 0$, we have

$$
\lim_{n\rightarrow\infty} P\left(\left| \overline{X} - \mu \right| > \epsilon\right) = 0
$$

<span class="statement-title">*Proof*.</span><br>

<div class="math-statement" markdonw="1">

\<Chebyshev's Inequality\>ë¥¼ ì‚¬ìš©í•˜ë©´ ì•„ì£¼ ì‰½ê²Œ ì¦ëª…í•  ìˆ˜ ìˆë‹¤!

$$
\begin{aligned}
P\left(\left| \overline{X} - \mu \right| > \epsilon\right) &\le \frac{\text{Var}(\overline{X})}{\epsilon^2} \\
&= \frac{1}{\epsilon} \cdot \frac{\sigma^2}{n} \rightarrow 0 \quad \text{as} \quad n \rightarrow \infty
\end{aligned}
$$

$\blacksquare$

</div>

<big>"WLLN says that as the sample size $n$ gets larger, then the sample mean is close to the true mean in probability!"</big>

ì´ë•Œ, WLLNê³¼ ê°™ì€ í˜•íƒœì˜ ìˆ˜ë ´ì„ <span class="half_HL">"the convergence in probability"</span>ë¼ê³  í•œë‹¤.

cf) ì°¸ê³ ë¡œ \<Strong Law of Larger Numbers\>ë„ ì¡´ì¬í•œë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ì •ë¦¬ë¥¼ ì¦ëª…í•˜ë ¤ë©´, ì¸¡ë„(measure)ì— ëŒ€í•œ ê°œë…ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— ì†Œê°œë§Œ í•˜ê³  ë„˜ì–´ê°€ê² ë‹¤.

$$
P\left(\lim_{n\rightarrow\infty} \overline{X} = \mu \right) = 1
$$

<hr/>

#### CLT; Central Limit Theorem

<span class="statement-title">Example.</span><br>

What is the probability of $P\left( \overline{X} > 7\right)$?

Note that $X_1, \dots, X_n \sim \text{Ber}(p)$, and then $(X_1 + \cdots + X_n) \sim \text{BIN}(n, p)$. 

When $n$ is larget, then $(X_1 + \cdots + X_n) \rightarrow N(\mu, \sigma^2)$.

Let's standardize it, then

$$
P\left( \frac{(X_1 + \cdots + X_n) - np}{\sqrt{npq}} \le z \right) \approx P(Z \le z)
$$

ì´ë•Œ, ì¢Œë³€ì˜ ë¶„ëª¨/ë¶„ìì— $n$ë¥¼ ë‚˜ì¤˜ì£¼ë©´

$$
P\left( \frac{((X_1 + \cdots + X_n) - np)/n}{(\sqrt{npq})/n} \le z \right) = P\left( \frac{\frac{(X_1 + \cdots + X_n)}{n} - p}{\sqrt{\frac{pq}{n}}} \le z \right) = P\left( \frac{\overline{X} - E[\overline{X}]}{\sqrt{\text{Var}(\overline{X})}} \le z \right) \approx P(Z \le z)
$$

ê·¸ë˜ì„œ ê²°ë¡ ì€, ì›ë˜ ë¬¸ì œì˜€ë˜ $P\left(\overline{X} > 7\right)$ì„ ì˜ ì •ê·œí™”í•´ì„œ normal ë¶„í¬ë¡œ ê·¼ì‚¬í•˜ì—¬ í’€ë©´ ëœë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ëŸ°ë°, ì§€ê¸ˆì˜ ê²½ìš°ëŠ” $\overline{X}$ê°€ BIN ë¶„í¬ì˜€ê¸° ë•Œë¬¸ì— \<Noarml Approximation\>ì— ì˜í•´ ìì—°ìŠ¤ëŸ½ê²Œ ìœ ë„ëœ ê²ƒì´ì—ˆë‹¤. ê³¼ì—° $\overline{X}$ ë˜ëŠ” $X_i$ê°€ ë‹¤ë¥¸ ë¶„í¬ë¥¼ ê°€ì ¸ë„ ìœ„ì™€ ê°™ì€ ë°©ì‹ì„ í’€ ìˆ˜ ìˆì„ê¹Œ? ì´ ì˜ë¬¸ì— ëŒ€í•œ ë‹µì„ ì œì‹œí•˜ëŠ” ê²ƒì´ ë°”ë¡œ \<CLT; Central Limit Theorem\>ì´ë‹¤ ğŸ¤©

<br/>

<span class="statement-title">Theorem.</span> CLT; Central Limit Theorem<br>

Let $X_1, \dots, X_n$ be a random sample with $E[X_i] = \mu$ and $\text{Var}(X_i) = \sigma^2$.

Let $\overline{X}_n := \dfrac{X_1 + \cdots + X_n}{n}$.

Let $Z_n := \dfrac{\overline{X}_n - E[\overline{X}]}{\sqrt{\text{Var}(\overline{X})}} = \dfrac{\overline{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}}$.

then, for any $z \in \mathbb{R}$, we have

$$
P(Z_n \le z) \rightarrow P(Z \le z) \quad \text{as} \quad n \rightarrow \infty
$$

where $Z \sim N(0, 1)$.

<span class="statement-title">Remark.</span><br>

1\. As long as i.i.d. RVs have finite second moment[^1], then we have CLT.

ì´ê²ƒì´ ì˜ë¯¸í•˜ëŠ” ë°”ëŠ” ì•„ì£¼ ê°•ë ¥í•˜ë‹¤ ğŸ’¥ ë°”ë¡œ $X_i$ê°€ ì–´ë–¤ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ìƒê´€ì—†ì´ CLTë¥¼ ì ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ë§ì´ê¸° ë•Œë¬¸ì´ë‹¤!! ì´ëŸ° ì  ë•Œë¬¸ì— CLTë¥¼ <span class="half_HL">"universal result"</span>ë¼ê³  í•œë‹¤!

<br/>

2\. We call $z: = \dfrac{\overline{x} - \mu}{\sigma / \sqrt{n}}$ as \<$z$-value\> or \<$z$-score; $z$-ì ìˆ˜, í‘œì¤€ ì ìˆ˜\>, we define $z_\alpha$ as the number $x$ s.t. $P(Z \ge x) = \alpha$ when $Z \sim N(0, 1)$.

<div class="img-wrapper">
<img src="https://upload.wikimedia.org/wikipedia/commons/2/25/The_Normal_Distribution.svg" height="400px">
</div>

\<CLT\>ì— ëŒ€í•œ ì¦ëª…ì€ ì¶”í›„ì— ì œì‹œí•˜ë„ë¡ í•˜ê² ë‹¤!

<hr/>

[^1]: ê·¸ëƒ¥ finite varianceë¥¼ ê°€ì§„ë‹¤ëŠ” ë§ì´ë‹¤.

