---
title: "Introduction to Linear Regression"
layout: post
use_math: true
tags: ["Statistics"]
---


2021-1í•™ê¸°, ëŒ€í•™ì—ì„œ 'í™•ë¥ ê³¼ í†µê³„' ìˆ˜ì—…ì„ ë“£ê³  ê³µë¶€í•œ ë°”ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

<br><span class="statement-title">TOC.</span><br>

- [Introduction to Linear Regression](#introduction-to-linear-regression)
- [Simple Linear Regression](#simple-linear-regression)
- [Least Square Method](#least-square-method)

<hr/>

### Introduction to Linear Regression

\<Regression\>ì„ ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•˜ìë©´, ì•„ë˜ì™€ ê°™ë‹¤.

<div class="statement" style="font-size: larger" align="center" markdown="1">

Model the relationship btw $x$ and $y$ <br/>
by finding a function $y = f(x)$ that is a close fit to the data

</div>

<span class="statement-title">Mathematical Model</span>

$$
Y = \beta_0 + \beta_1 x + \epsilon
$$

ì´ë•Œ, regression parameter $\beta_0$, $\beta_1$ê°€ unknownì´ë©°, ìš°ë¦¬ëŠ” ì´ê²ƒì„ sample pointë¡œë¶€í„° ì¶”ì •(Estimate)í•  ê²ƒì´ë‹¤!

\* $\epsilon$ì€ errorë¥¼ ëŒ€ë³€í•˜ë©°, random variableì´ë‹¤!

<hr/>

### Simple Linear Regression

<div class="definition" markdown="1">

<span class="statement-title">Definition.</span> Simple Linear Regression Model<br>

$n$ê°œ sample pointsë¥¼ ê°–ê³  ìˆë‹¤ê³  í•˜ì: $(x_1, y_1), \dots, (x_n, y_n)$.

$y_i$ê°€ $x_i$ì— dependent í•˜ë‹¤ê³  ê°€ì •í•œë‹¤. ì´ë•Œ, ë‘˜ì€ random factorì— ì˜í•´ ì˜í–¥ì„ ë°›ëŠ”ë‹¤. ì´ random factorëŠ” $\epsilon_i$ë¡œ í‘œí˜„ëœë‹¤.

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i
$$

where $\epsilon_i$ are independent random variables with mean 0 and variance $\sigma^2$.

ìœ„ì™€ ê°™ì€ Regression Modelingì„ \<**Simple Linear Regression Model**\>ì´ë¼ê³  í•œë‹¤!!

</div>


<div class="statement" markdown="1">

<span class="statement-title">Remark.</span><br>

1\. $x_i$ is called the \<predictor\> or \<regressor\>, and <span style="color: red">we assume $x_i$s are non-random.</span>

2\. $y_i$ is called the \<response\>, and it is a random variable with $E[y_i] = \beta_0 + \beta_1 x_i$ and $\text{Var}(y_i) = \sigma^2$.

3\. $\epsilon_i$ is called an \<error\>, and $\sigma^2$ is called the \<**error variance**\>. ğŸ”¥

</div>

ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ì´ëŸ° ì˜ë¬¸ì´ ë“ ë‹¤!

Q. ìš°ë¦¬ëŠ” ì£¼ì–´ì§„ data pointsì— ë§ëŠ” line $y = \beta_0 + \beta_1 x$ë¥¼ ì°¾ê³ ì‹¶ë‹¤. ì´ë•Œ, $\beta_0$, $\beta_1$ë¡œ ê°€ëŠ¥í•œ ê°’ì´ ì•„ì£¼ ë§ì„ í…ë°, ì–´ë–¤ $\beta_0$, $\beta_1$ ê°’ì´ ì¢‹ë‹¤ê³  ë§í•  ìˆ˜ ìˆì„ê¹Œ??

ìš°ë¦¬ëŠ” ì´ "Linear Regression"ì˜ ì¢‹ì€ ì •ë„ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ \<residual\>ê³¼ ê·¸ë“¤ì˜ í•©ì¸ \<residual sum\>ì„ ì •ì˜í•œë‹¤.

<div class="definition" markdown="1">

<span class="statement-title">Definition.</span> residual<br>

For a line $\hat{y} = b_0 + b_1 x$, the \<residual\> $e_i$  of a data point $(x_i, y_i)$ is defined to be 

$$
e_i := y_i - \hat{y}_i
$$

</div>

ìš°ë¦¬ëŠ” ì´ residualì„ ìµœì†Œí™”í•˜ëŠ” $b_0$, $b_1$ì˜ ê°’ì„ ì°¾ê³  ì‹¶ë‹¤!! ì´ë•Œ, ì“°ëŠ” ë°©ë²•ì´ ë°”ë¡œ \<Least Square Method\>ë‹¤!

<hr/>

### Least Square Method

\<LS Method\>ëŠ” ìµœì„ ì˜ $\beta_0$, $\beta_1$ì„ ì–»ê¸° ìœ„í•´ ì•„ë˜ì˜ \<SSE; Sum of Squares of the Errors\>ë¥¼ ìµœì†Œí™”í•˜ëŠ” $b_0$, $b_1$ë¥¼ ì°¾ê³ ì í•œë‹¤!

$$
\text{SSE} = \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2
$$

ìœ„ì˜ ì‹ì„ ìµœì í™”í•˜ëŠ” ê±´ ì •ë§ ê°„ë‹¨í•˜ë‹¤. ê·¸ëƒ¥ SSEë¥¼ $b_0$, $b_1$ì— ëŒ€í•´ í¸ë¯¸ë¶„ í•´ì„œ 0ì´ ë˜ëŠ” $b_0$, $b_1$ì˜ ê°’ì„ ì°¾ìœ¼ë©´ ëœë‹¤.

Let $f(b_0, b_1) = \text{SSE}$, then

$$
\frac{\partial f}{\partial b_0} = - 2 \sum_{i=1}^n (y_i - b_0 - b_1 x_i) = 0
$$

$$
\frac{\partial f}{\partial b_1} = - 2 \sum_{i=1}^n (y_i - b_0 - b_1 x_i) x_i = 0
$$

ë¨¼ì €, $b_0$ì— ëŒ€í•œ ì‹ë¶€í„° ì •ë¦¬í•´ë³´ì.

<div class="statement" markdown="1">

$$
\frac{\partial f}{\partial b_0} = - 2 \sum_{i=1}^n (y_i - b_0 - b_1 x_i) = 0
$$

$$
\sum_{i=1}^n (y_i - b_0 - b_1 x_i) = 0
$$

$$
\sum_{i=1}^n y_i = b_0 \sum_{i=1}^n 1 + b_1 \sum_{i=1}^n x_i
$$

ì–‘ë³€ì„ $n$ìœ¼ë¡œ ë‚˜ëˆ„ë©´,

$$
\bar{y} = b_0 + b_1 \bar{x}
$$

ë”°ë¼ì„œ,

$$
b_0 = \bar{y} - b_1 \bar{x}
$$

$\blacksquare$

</div>

ì´ë²ˆì—ëŠ” $b_1$ì— ëŒ€í•œ ì‹ì„ ì •ë¦¬í•´ë³´ì.

<div class="statement" markdown="1">

$$
\frac{\partial f}{\partial b_1} = - 2 \sum_{i=1}^n (y_i - b_0 - b_1 x_i) x_i = 0
$$

$$
\sum_{i=1}^n (y_i - b_0 - b_1 x_i) x_i = 0
$$

ìœ„ì˜ ì‹ì—ì„œ ì•„ê¹Œ êµ¬í•œ $b_0$ë¥¼ ëŒ€ì…í•´ì£¼ì!

$$
\sum_{i=1}^n (y_i - \bar{y} + b_1 \bar{x} - b_1 x_i) x_i = 0
$$

$$
\sum_{i=1}^n (y_i - \bar{y} + b_1 (\bar{x} - x_i)) x_i = 0
$$

$$
\sum_{i=1}^n (y_i - \bar{y})x_i + \sum_{i=1}^n b_1 (\bar{x} - x_i) x_i= 0
$$

$$
b_1 \cdot \sum_{i=1}^n (\bar{x} - x_i) x_i= - \sum_{i=1}^n (y_i - \bar{y})x_i
$$

$$
b_1 = - \frac{\sum (y_i - \bar{y})x_i}{\sum (\bar{x} - x_i) x_i}
$$

$$
b_1 = \frac{\sum (y_i - \bar{y})x_i}{\sum (x_i - \bar{x}) x_i}
$$

ë˜ëŠ” ìœ„ì˜ ì‹ì„ ì•½ê°„ ë³€í˜•í•´ ì•„ë˜ì™€ ê°™ì´ ì“°ê¸°ë„ í•œë‹¤.

$$
b_1 = \frac{\sum (y_i - \bar{y})(x_i - \bar{x})}{\sum (x_i - \bar{x}) (x_i - \bar{x})}
$$

ì´ê²Œ ê°€ëŠ¥í•œ ê²ƒì€ $b_1$ì— ëŒ€í•œ ì²«ë²ˆì§¸ ì‹ì—ì„œ $\sum (y_i - \bar{y}) \bar{x}$, $\sum (x_i - \bar{x}) \bar{x}$ë¥¼ ë¹¼ì¤„ ë•Œ, $\sum (y_i - \bar{y}) = \sum (x_i - \bar{x}) = 0$ì´ê¸° ë•Œë¬¸ì´ë‹¤!!

</div>

ë‹¤ì‹œ ì˜ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

<div class="theorem" markdown="1">

In \<LS method\>, the regression coefficients of $\beta_0$ and $\beta_1$ are estimated by

$$
b_1 = \frac{\sum (y_i - \bar{y})(x_i - \bar{x})}{\sum (x_i - \bar{x}) (x_i - \bar{x})} = \frac{S_{xy}}{S_{xx}}
$$

$$
b_0 = \bar{y} - b_1 \bar{x}
$$

</div>

ì—¬ê¸°ê¹Œì§€ ì§„í–‰í•˜ë©´, ì´ì œ ì•„ë˜ì™€ ê°™ì€ ì˜ë¬¸ì´ ë“ ë‹¤.

Q. Are $b_1$ and $b_0$ good estimators? ğŸ¤”

A. Yes!!

<div class="theorem" markdown="1">

<span class="statement-title">Theorem.</span><br>

$b_1$ and $b_0$ are unibased for $\beta_1$ and $\beta_0$ respectively.

$$
E[b_1] = \beta_1 \quad \text{and} \quad E[b_0] = \beta_0 
$$

</div>


<div class="proof" markdown="1">

<span class="statement-title">*proof*.</span><br>

$$
\begin{aligned}
E[b_1]
&= E \left[ \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{S_{xx}} \right] \\
&= E \left[ \frac{\sum_{i=1}^n (x_i - \bar{x})y_i}{S_{xx}} \right] \\
&= \frac{\sum_{i=1}^n (x_i - \bar{x})E[y_i]}{S_{xx}}
\end{aligned}
$$

ì‹ì—ì„œ ìœ„ì™€ ê°™ì´ $E[y_i]$ê°€ ê°€ëŠ¥í•œ ì´ìœ ëŠ” <span style="color: red">$x_i$ëŠ” Random Variableì´ ì•„ë‹ˆê¸° ë•Œë¬¸</span>ì´ë‹¤!! 

$$
\begin{aligned}
&= \frac{\sum_{i=1}^n (x_i - \bar{x})E[y_i]}{S_{xx}} \\
&= \frac{\sum_{i=1}^n (x_i - \bar{x})(\beta_0 + \beta_1 x_i )}{S_{xx}} \\
&= \frac{\cancel{\sum_{i=1}^n (x_i - \bar{x})\beta_0} + \sum_{i=1}^n (x_i - \bar{x}) \beta_1 x_i }{S_{xx}} \\
&= \frac{\sum_{i=1}^n (x_i - \bar{x}) \beta_1 x_i }{S_{xx}} \\
&= \beta_1 \cdot \frac{\sum_{i=1}^n (x_i - \bar{x}) x_i }{S_{xx}} \\
&= \beta_1 \cdot \cancelto{1}{\frac{\sum_{i=1}^n (x_i - \bar{x}) (x_i - \bar{x})}{S_{xx}}} \\
&= \beta_1
\end{aligned} 
$$

$\blacksquare$

</div>

<div class="proof" markdown="1">

<span class="statement-title">*proof*.</span><br>

$$
\begin{aligned}
E[\beta_0]
&= E[\bar{y} - b_1 \bar{x}] \\
&= E[\bar{y}] - E[b_1] \cdot \bar{x} \\
&= (\beta_0 + \beta_1 \bar{x}) - \beta_1 \bar{x} \\
&= \beta_0
\end{aligned}
$$

$\blacksquare$

</div>

<div class="statement" markdown="1">

<span class="statement-title">Remark.</span><br>

1\. The derivation of LSEs does not depend on the distribution of $\epsilon_i$.

2\. If $\epsilon_i$s are iid $N(0, \sigma^2)$, then $b_0$ and $b_1$ are the MLEs for $\beta_0$ and $\beta_1$.

3\. $\sum e_i = 0$

4\. $\sum x_i e_i = 0$

(Homework ğŸˆ)

</div>

ìœ„ì˜ ëª…ì œ [3, 4]ë¥¼ í™œìš©í•´ ì•„ë˜ì˜ ë“±ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.

$$
\sum_{i=1}^n (y_i - \bar{y})^2 = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2 + \sum_{i=1}^n (y_i - \hat{y}_i)^2
$$

ì´ë•Œ, ê° í…€ì€ ì•„ë˜ì™€ ê°™ë‹¤.

$$
\text{SST} = \text{SSR} + \text{SSE}
$$

- SST: Total Sum of Squares
- SSR: the Regression Sum of Squares
- SSE: the Residual Sum of Squares

ì¦ëª…ì€ ë§ˆì°¬ê°€ì§€ë¡œ (Homework ğŸˆ)

<hr/>

<div class="definition" markdown="1">

<span class="statement-title">Definition.</span> R-square; ê²°ì • ê³„ìˆ˜<br>

$$
R^2 := 1 - \frac{\text{SSE}}{\text{SST}}
$$

be the "coefficient of determination; ê²°ì • ê³„ìˆ˜".

- $R^2 = 1$ is equivalent to
  - $\text{SSE} = 0$
  - $\hat{y_i} = y_i$ for all inputs
  - Regression model work very well!
- $R^2 = 0$ is equivalent to
  - $\text{SSE} = \text{SST}$
  - $\text{SSR} = 0$
  - $\hat{y}_i = \bar{y}$ for all inputs
  - Regression model outputs a constant.

</div>


<div class="definition" markdown="1">

<span class="statement-title">Remark.</span><br>

1\. $0 \le R^2 \le 1$

2\. $R^2$ represents the proportionate reduction of total variation in $Y$ associated with the use of the variable $X$.

(a) If $R^2=1$, then $SSE = 0$, this means $\hat{y}_i = y_i$.

All observations fall on the line.

(b) If $R^2 = 0$, then $\text{SSE} = \text{SST}$ or $\text{SSR} = 0$.

The fitted regression line is the constant, $\bar{y}$.

</div>

<hr/>

ì´ì–´ì§€ëŠ” í¬ìŠ¤íŠ¸ì—ì„œëŠ” \<Simple Linear Regression\>ì˜ ì„±ì§ˆì„ì„ ì´ì–´ì„œ ì‚´í´ë³¼ ì˜ˆì •ì´ë‹¤. \<Linear Regression\>ì—ì„œ ê³„ìˆ˜ $b_0$, $b_1$ì˜ ë¶„í¬ë¥¼ ì‚´í´ë³´ê³  ì´ë¥¼ í†µí•´ ê²€ì •(Test)ì„ ìˆ˜í–‰í•œë‹¤. ë˜, Regressionì„ í†µí•´ ì–»ì€ Prediction ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ \<Prediction Inference\>ë¥¼ ìˆ˜í–‰í•œë‹¤!

ğŸ‘‰ [Test on Regression]({{"/2021/06/09/test-on-regression.html" | relative_url}})